%
% File naaclhlt2018.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{soul}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Title: Something snappy about the SLAM competition}


\author{Alexander S. Rich\qquad Pamela Osborn Popp\qquad David J. Halpern\\
  \textbf{Anselm Rothe\qquad Todd M. Gureckis} \\
  Department of Psychology, New York University \\
  {\tt \{asr443,pamop,david.halpern,anselm,todd.gureckis\}@nyu.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}

Blah Blah \cite{slam18}

\section{Task Approach}

We approached the task as a binary classification problem over instances (i.e.,
single words within an exercise). Our solution can be divided into two
components---constructing a set of features that is highly informative about
whether the user will answer an instance correctly, and designing a model that
can achieve high performance using this feature set.

\subsection{Feature Engineering}

We used a variety of features, including features directly present in the
training data, features constructed using the training data, and features that
use information external to the training data. Except where otherwise specified,
categorical variables were one-hot encoded.

\subsubsection{Exercise features}

We encoded the exercise client, session, format, and time (i.e., number of
seconds to complete the exercise), as well as the exercise number and number of
days since start of usage.

\subsubsection{Word features}

Using spaCy\footnote{https://spacy.io/}, we lemmatized each word to produce a root word. Both the root
word token and the original token were used as categorical features. Due to
their high cardinality, these features were not one-hot encoded but were
preserved in single columns and handled in this form by the model (as described
below).

Along with the tokens themselves we encoded \hl{each} instance word's part of speech,
morphological features, and dependency edge label. (We noticed that some words
in the original dataset were paired with the wrong morphological features,
particularly near where punctuation had been removed from the sentence. To fix
this, we re-processed the data using Google SyntaxNet\footnote{https://github.com/ljm625/syntaxnet-rest-api}.)

We also encoded word length and several word characteristics gleaned from
external data sources. The word frequency effect suggests that uncommon words are harder to process than common words; readers will look longer at low-frequency words and perform worse in word-identification tasks for these than for high-frequency words \cite{rayner1998eye}. We therefore included a feature that encoded the frequency of each word in the language being acquired, calculated from \citet{robert_speer_2017_998161}. Additionally, cognates, or words sharing a common linguistic derivation, are easier to learn than words with dissimilar translations \cite{de2000hard}. As an approximate measure of linguistic similarity, we used the Levenshtein or edit distance between the word tokens and their translations scaled by the length of the longer word. We found translations using Google Translate\footnote{https://cloud.google.com/translate/} and calculated the Levenshtein distance to reflect the letter-by-letter similarity of the word and its translation \cite{hyyro2001explaining}. \hl{DAVID FILL IN HERE WITH AOA}. 

\subsubsection{User features}

Just as we did for word tokens, we encoded the user ID as a single-column,
high-cardinality feature. We also calculated several other user-level features.
\hl{TODD AND ANSELM HERE}

\subsubsection{Positional features}

To account for the effects of surrounding words on the difficulty of an
instance, we created several features related to the instance word's context in
the exercise. These included the token of the previous word, the next word, and
the instance word's root in the parse tree, all stored in single columns as with
the instance token itself. We also included the part of speech of each of these
context words as additional features. When there was no previous word, next word, or parse
root word, a special {\tt None} token or {\tt None} part of speech was used.

\subsubsection{Temporal features}

A user's probability of succeeding on an instance is likely related to their
prior experience with that instance. To capture this, we calculated several
features related to past experience. We encoded the number of times the
current exercise's exact sentence had been seen before by the user. \hl{is this
  right Todd?} We also encoded a set of features recording past experience with
the particular instance word. These features were encoded separately for the
instance token and for the instance root word created by lemmatization.

For each token (and root) we tracked user performance through four weighted
error averages. At the user's first encounter of the token, each error term $E$ starts at
zero. After an encounter with an instance of the token with label $L$, it is
updated according to the equation

\[
E \leftarrow E + \alpha (L - E)
\]

where $\alpha$ determines the speed of error updating. The four feature weighted
error terms use $\alpha = \{.3, .1, .03, .01\}$, allowing both short-run and
long-run changes in a user's error rate with a token to be tracked. Note that in
cases where a token appears multiple times in an exercise, a single update of
the error features is conducted using the mean of the token labels.
Along with the error tracking features, for each token we calculated the number
of labeled, unlabeled, and total encounters; time since last labeled encounter and
last encounter; and whether the instance is the first encounter with the
token.

In the training data, all instances are labeled as correct or incorrect, so the
label for the previous encounter is always available. In the test data, labels
are unavailable, so predictions must be made using a mix of labeled and
unlabeled past encounters. To generate training-set features that are 
comparable to test-set features, we selectively ignored some labels when encoding temporal features on
the training set. Specifically, for each user we first calculated the number of
exercises $n$ in the true test set. Then, when encoding the features for each
instance, we selected a random integer $r$ in the range $[1,n]$, and ignored labels
in the prior $r$ exercises. That is, we encode features for the current instance
as though other instances in those prior exercises were unlabeled, and ignore
updates to the error averages from those exercises. The result of this process
is that each instance in the training set is encoded as though it were between
one and $n$ exercises into the test set.

\subsection{Modeling}

After featurizing the training data, we trained gradient boosting decision
tree (GBDT) models to minimize log loss. GBDT works by
iteratively building regression trees, each of which seeks to minimize the
residual loss from prior trees. This allows it to capture non-linear effects
and high-order interactions among features. We used the LightGBM\footnote{http://lightgbm.readthedocs.io/} implementation
of GBDT \cite{ke2017lightgbm}.

For continuous-valued features, GBDT can split a leaf at any point, creating
different predicted values above and below that threshold. For categories that
are one-hot encoded, it can split a leaf on any of the category's features. This
means that for a category with thousands of values, potentially thousands of
tree splits would be needed to capture its relation to the target. Fortunately,
LightGBM implements an algorithm for partitioning the values of a categorical
feature into two groups based on their relevence to the current loss, and create
a single split to divide those groups \cite{fisher1958grouping}. Thus, as
alluded to above, high-cardinality features like token and user were encoded as
single columns and handled as categories by LightGBM.

We trained a model for each of the three language tracks of {\tt en\_es}, {\tt es\_en},
and {\tt fr\_en}, and also trained a model on the combined data from
all three tracks, adding an additional ``language'' feature. Following model
training, we averaged the predictions of each single-language model with that of
the all-language model to form our final predictions.

To tune model hyper-parameters and evaluate the usefulness of features, we first
trained the models on the ``train'' data set and evaluated them on the ``dev''
data set. Once the model structure was finalized, we trained on the combined
``train'' and ``dev'' data and produced predictions for the ``test'' data. The
LightGBM hyperparameters used for each model are listed in Table~\ref{lightgbm-params}.


\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|llll|}
  % \hline   & \multicolumn{3}{l}{\bf Value} & \\
  \hline \bf Parameter & {\tt fr\_en} & {\tt en\_es} & {\tt es\_en} & {\tt all} \\ \hline
  num\_leaves & 256 & 512 & 512 & 1024 \\
  learning\_rate & .05 & .05 & .05 & .05 \\
  min\_data\_in\_leaf & 100 & 100 & 100 & 100 \\
  num\_boost\_rounds & 750 & 650 & 600 & 750 \\
  cat\_smooth & 200 & 200 & 200& 200 \\
  feature\_fraction & .7 & .7 & .7 & .7 \\
  max\_cat\_threshold & 32 & 32 & 32& 64 \\

\hline
\end{tabular}
\end{center}
\caption{\label{lightgbm-params} Parameters of final LightGBM models. See
  LightGBM documentation for more information; all other parameters were left at
their default values.}
\end{table}

\subsection{Performance}

The AUROC of our final predictions was $.8585$ on {\tt en\_es}, $.8350$ on {\tt
  es\_en}, and $.8540$ on {\tt fr\_en}. We did not attempt to optimize the model's
F1 score, but the model's F1 score could likely be improved (at the cost of
increased log loss) by finding the
rescaling of the ``dev'' predicted probabilities that maximized the F1 score at
the 0.5 threshold, and applying this rescaling to the ``test'' predicted probabilities.

\section{Feature Removal Experiments}

\bibliography{paper}
\bibliographystyle{acl_natbib}

\end{document}
