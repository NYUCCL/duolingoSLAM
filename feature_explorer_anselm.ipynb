{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duolingo SLAM Explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are focus on creating new features to use with the gradient boosted trees (microsoft lightgbm):\n",
    "\n",
    "---\n",
    "So far Alex created features that fall into these categories:\n",
    "\n",
    "**Basic word features:**  \n",
    "These are a bit like the mental lexicon.  Definitions, stuff you look up in wordnet.  Noun? Verb?\n",
    "Plural?, etc...  Many came for free from the dataset itself and we aren't sure about adding too much more here. We aren't word people anyway.\n",
    "- word length\n",
    "- morphological features\n",
    "- tokenid (one-hot word index)\n",
    "\n",
    "**Position/sequence features:**  \n",
    "These are sort of like grammatical aspect because captures something about sequential structure.\n",
    "- previous word part of speeach\n",
    "- next word part of speech\n",
    "- root word part of speach \n",
    "\n",
    "**User features:**  \n",
    "Features about the users themselves.\n",
    "- userid (one-hot user index)\n",
    "\n",
    "**Temporal features (per word):**  \n",
    "- number of observation of a word (total, unlabeled, labeled)\n",
    "- time since last observation (lab, unlabeled)\n",
    "- exponentially smoothed running average of probabily of remembering (4 different fixed rates).  no here decay in absense of information\n",
    "- is it 1st encounter with word? (true/false)\n",
    "\n",
    "**Semantic features:**  \n",
    "Not sure if these are particularly useful here.  Something about word meaning, similarities in meanings, e.g., positive or negative word, emotion?, some might be in the basic features, etc...\n",
    "- none currently\n",
    "\n",
    "---\n",
    "Plan of attack for this weekend:  \n",
    "1. [ ] Focus on user features (more information about user motivation, session structure, etc...).  (**Anselm is pursuing this**)\n",
    "1. [ ] Focus on temporal features that capture spaced/massed practice.  (**Alex is pursuing this**) \n",
    "1. [ ] Model something about context (repeated contexts aid memory) (**Todd is pursuing this**)\n",
    "1. [ ] Cognates and word similarity both in terms of letters and meaning (**Pam is pursuing this**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from processing import build_data\n",
    "import pandas as pd\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "#import lightgbm as lgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for longer computations\n",
    "from IPython.display import Audio\n",
    "notification = 'notification.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "You can control what language you are messing with here: options are `all`, `en_es` (reverse spanish), `fr_en` (french), `es_en` (spanish)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to change language pair trained on\n",
    "lang = 'en_es'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "The main script for parsing and constructing features is `processing.py`.  You should edit it in a different editor (e.g., sublime) and then run the cell below to re-load it into this jupyter kernel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data files\n",
      "retrieving labels\n",
      "building features\n"
     ]
    }
   ],
   "source": [
    "# configuration options\n",
    "# NUSERS = 10. # set this to None to load all the users for the given language\n",
    "NUSERS = None # set this to None to load all the users for the given language\n",
    "FEATUREIZED = False # set this to true to return the features as dict() instead of instances of the User() class\n",
    "\n",
    "# load data\n",
    "if lang == 'all':\n",
    "    data = build_data(\n",
    "        'all',\n",
    "        [\n",
    "            'data/data_{0}/{0}.slam.20171218.train.new'.format('en_es'),\n",
    "            'data/data_{0}/{0}.slam.20171218.train.new'.format('fr_en'),\n",
    "            'data/data_{0}/{0}.slam.20171218.train.new'.format('es_en')\n",
    "        ],\n",
    "        [\n",
    "            'data/data_{0}/{0}.slam.20171218.dev.new'.format('en_es'),\n",
    "            'data/data_{0}/{0}.slam.20171218.dev.new'.format('fr_en'),\n",
    "            'data/data_{0}/{0}.slam.20171218.dev.new'.format('es_en')\n",
    "        ],\n",
    "        labelfiles=[\n",
    "            'data/data_{0}/{0}.slam.20171218.dev.key'.format('en_es'),\n",
    "            'data/data_{0}/{0}.slam.20171218.dev.key'.format('fr_en'),\n",
    "            'data/data_{0}/{0}.slam.20171218.dev.key'.format('es_en')\n",
    "        ],\n",
    "        n_users=NUSERS, featurized=FEATUREIZED)\n",
    "else:\n",
    "    data = build_data(\n",
    "        lang[:2],\n",
    "        ['data/data_{0}/{0}.slam.20171218.train.new'.format(lang)],\n",
    "        ['data/data_{0}/{0}.slam.20171218.dev.new'.format(lang)],\n",
    "        labelfiles=['data/data_{0}/{0}.slam.20171218.dev.key'.format(lang)],\n",
    "        n_users=NUSERS, featurized=FEATUREIZED)\n",
    "train_x, train_ids, train_y, test_x, test_ids, test_y = data\n",
    "\n",
    "Audio(url=notification, autoplay=True)  # play sound for longer computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran with `FEATUREIZED = False` then the following cells will let you explore individual users:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the basic data structures programatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the user id and the languge out of the user object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].id, train_x[0].features['user'], train_x[0].features['lang']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New: get the user usage entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of the exercises this user completed each as a Exercise() instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first exercise this person did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].exercises[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the raw text of the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].exercises[0].textlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the other features defined on the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].exercises[24].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each exercise has a list of Instance() instances which are are python structured representation of the entried of the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].exercises[0].instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which itself has a lot of features many which are akin to the basic and position features described above, but also including temporal features such as `root:erravg0` which is keeping track of a exponential smoothed average of error probability, etc...  \n",
    "\n",
    "note: these same things aren't all present for the first items in the exercise list (this is showing the last item) because error average isn't yet defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[0].exercises[-1].instances[0].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Examining user properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Days is not a feature currently but is a value in the header of each exercise that says how long since the person started duolingo the current exercise was completed.  These intervals might index something about user engagement or consisteny and so might be interesting.  The following cell step through an example so you can see how to analyze and possibly add this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This plots a scatter plot where the x location of the point is the time in days since the first session (so whole numbers are 24 intervals).   If you try different `user_number` values you can see how different people used it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "user_number = 0\n",
    "x_days = [train_x[user_number].exercises[i].days for i in range(len(train_x[0].exercises))]\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = x_days,\n",
    "    y = [1.0]*len(x_days),\n",
    "    mode='markers',\n",
    "    marker=dict(opacity=0.2)\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "plot(data, filename='basic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This does a modulo on the number of days for each session then plots the resulting data as a histogram.  Most of the plots then show a bi-modal distribution which is nightime.  Is there something interesting there about consistency in time and performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "user_number = 5\n",
    "x_days = [train_x[user_number].exercises[i].days%1.0 for i in range(len(train_x[user_number].exercises))]\n",
    "\n",
    "\n",
    "data = [go.Histogram(x=x_days)]\n",
    "\n",
    "plot(data, filename='basic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is everyone we loaded originaly (NUSERS) together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "times = []\n",
    "for user_number in range(len(train_x)):\n",
    "    for i in range(len(train_x[user_number].exercises)):\n",
    "        times.append(train_x[user_number].exercises[i].days%1.0)\n",
    "    \n",
    "\n",
    "data = [go.Histogram(x=times)]\n",
    "\n",
    "plot(data, filename='basic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User features (Anselm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data with `NUSERS = None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a u-shaped distribution. Explanation: Day = 0.00 is the moment they started using the app, not midnight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for user_number in range(len(train_x)):\n",
    "    for i in range(len(train_x[user_number].exercises)):\n",
    "        times.append(train_x[user_number].exercises[i].days%1.0)\n",
    "        \n",
    "data = [go.Histogram(x=times)]\n",
    "\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User experience = number of exercises\n",
    "This has been added as a user feature to processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute entropy for each user\n",
    "This has been added as a user feature to processing.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_usage_entropy(user_number, train_x):\n",
    "    x_days = [train_x[user_number].exercises[i].days%1.0 for i in range(len(train_x[user_number].exercises))]\n",
    "    x_bins = [round(x * 24 * 3) for x in x_days]  # 20-minutes bins\n",
    "    freq = Counter(x_bins)\n",
    "    rel_freq = [freq[key]/len(x_bins) for key in freq]\n",
    "    return(entropy(rel_freq, base=2))\n",
    "\n",
    "x = [compute_usage_entropy(user_number, train_x) for user_number in range(len(train_x))]\n",
    "data = [go.Histogram(x=x)]\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variability in accuracy for each user\n",
    "This has been added as a user feature to processing.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compute_accuracy_variance(user_number, train_x):\n",
    "    x_accuracy = [e.accuracy for e in train_x[user_number].exercises if not e.test]\n",
    "    return(np.var(x_accuracy))\n",
    "\n",
    "compute_accuracy_variance(0, train_x)\n",
    "\n",
    "x = [compute_accuracy_variance(user_number, train_x) for user_number in range(len(train_x))]\n",
    "data = [go.Histogram(x=x)]\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuing after mistakes\n",
    "This has been added as a user feature to processing.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_cor_mistakes_break(user_number, train_x):\n",
    "    exercise_mistakes = []\n",
    "    exercise_days_diff = []\n",
    "    days = 0\n",
    "    for e in train_x[user_number].exercises:\n",
    "        mistakes = len([1 for i in e.instances if not i.label == 0])\n",
    "        exercise_mistakes.append(mistakes)\n",
    "        days_diff = e.days - days\n",
    "        days = copy.deepcopy(e.days)  # save for next exercise\n",
    "        exercise_days_diff.append(days_diff)\n",
    "    cor = pearsonr(exercise_mistakes, exercise_days_diff)[0]\n",
    "    return(cor)\n",
    "\n",
    "compute_cor_mistakes_break(0, train_x)\n",
    "x = [compute_cor_mistakes_break(user_number, train_x) for user_number in range(len(train_x))]\n",
    "data = [go.Histogram(x=x)]\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single user example\n",
    "user_number = 111\n",
    "exercise_mistakes = []\n",
    "exercise_days_diff = []\n",
    "days = 0\n",
    "for e in train_x[user_number].exercises:\n",
    "    mistakes = len([1 for i in e.instances if not i.label == 0])\n",
    "    exercise_mistakes.append(mistakes)\n",
    "    days_diff = e.days - days\n",
    "    days = copy.deepcopy(e.days)  # save for next exercise\n",
    "    exercise_days_diff.append(days_diff)\n",
    "iplot(go.Figure(data=[go.Scatter(\n",
    "    x = exercise_mistakes,\n",
    "    y = exercise_days_diff,\n",
    "    mode = 'markers'\n",
    ")], layout=go.Layout(xaxis=dict(title='exercise_mistakes'), yaxis=dict(title='exercise_days_diff'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[user_number].exercises[0].days\n",
    "pearsonr([1,2,3],[0,2,3])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User experience vs variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [user.features['experience'] for user in train_x]\n",
    "y = [compute_accuracy_variance(user_number, train_x) for user_number in range(len(train_x))]\n",
    "\n",
    "data = [go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = 'markers',\n",
    "    marker = {'opacity':.2}\n",
    ")]\n",
    "layout = go.Layout(xaxis=dict(title='n_exercises'), yaxis=dict(title='variability'))\n",
    "iplot(go.Figure(data=data, layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_number = 0\n",
    "x = list(range(len(train_x[user_number].exercises)))\n",
    "y = [train_x[user_number].exercises[i].accuracy for i in x]\n",
    "iplot([{\"x\": x, \"y\": y}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histgram of exercise accuracies (0 = 100% correct):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for user_number in range(len(train_x)):\n",
    "    for i in range(len(train_x[user_number].exercises)):\n",
    "        x.append(train_x[user_number].exercises[i].accuracy)\n",
    "        \n",
    "data = [go.Histogram(x=x)]\n",
    "\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
